"""
Unified Open-Ended Interview Handler
PROMPT #78 - Unified Open-Ended Interview System

All interviews now use the same open-ended question format:
- Questions are open-ended (like GPT), not fixed
- AI generates questions freely based on context
- Response options are SUGGESTIONS, not requirements
- User can respond freely with text or click suggestions

This replaces all the fixed question modes (meta_prompt, requirements, task_focused, etc.)
"""

from typing import Dict, Any, Optional
from uuid import UUID
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy.orm.attributes import flag_modified
from fastapi import HTTPException, status
import logging

from app.models.interview import Interview
from app.models.project import Project
from app.models.task import Task
from app.services.ai_orchestrator import AIOrchestrator
from app.services.interview_question_deduplicator import InterviewQuestionDeduplicator
from app.api.routes.interviews.option_parser import parse_ai_question_options
from app.api.routes.interviews.response_cleaners import clean_ai_response
from app.api.routes.interviews.context_builders import prepare_interview_context

logger = logging.getLogger(__name__)


def build_unified_open_prompt(
    project: Project,
    interview: Interview,
    message_count: int,
    parent_task: Optional[Task] = None,
    previous_answers: Optional[Dict] = None
) -> str:
    """
    Build the system prompt for unified open-ended interviews.

    PROMPT #78 - Unified Open-Ended Interview System

    Key principles:
    1. Questions are OPEN-ENDED (like GPT)
    2. User can respond FREELY
    3. AI can offer SUGGESTIONS (optional)
    4. No fixed questions - AI decides what to ask

    Args:
        project: Project instance
        interview: Interview instance
        message_count: Current message count
        parent_task: Optional parent task for hierarchical interviews
        previous_answers: Dict of previous answers

    Returns:
        System prompt string
    """
    previous_answers = previous_answers or {}
    question_number = (message_count // 2) + 1

    # Build project context
    project_context = f"""
**PROJETO:**
- Nome: {project.name or 'NÃ£o definido'}
- DescriÃ§Ã£o: {project.description or 'NÃ£o definida'}
"""

    # Add stack context if available
    if project.stack_backend:
        project_context += f"""
**STACK TÃ‰CNICA:**
- Backend: {project.stack_backend}
- Database: {project.stack_database or 'NÃ£o definido'}
- Frontend: {project.stack_frontend or 'NÃ£o definido'}
- CSS: {project.stack_css or 'NÃ£o definido'}
- Mobile: {project.stack_mobile or 'NÃ£o definido'}
"""

    # Add parent task context for hierarchical interviews
    parent_context = ""
    if parent_task:
        parent_context = f"""
**CARD PAI:**
- Tipo: {parent_task.item_type or 'Task'}
- TÃ­tulo: {parent_task.title}
- DescriÃ§Ã£o: {parent_task.description or 'NÃ£o definida'}
"""

    # PROMPT #81 - EXTREMELY explicit prompt for CLOSED questions
    system_prompt = f"""ğŸš¨ ATENÃ‡ÃƒO: Use APENAS "â—‹" para opÃ§Ãµes! PROIBIDO usar "â€¢" ou "ğŸ’¡"! ğŸš¨

VocÃª Ã© um Product Owner experiente conduzindo uma entrevista para coletar requisitos de software.

{project_context}
{parent_context}

**FORMATO OBRIGATÃ“RIO (copie exatamente):**
```
â“ Pergunta {question_number}: [Pergunta FECHADA aqui]

â—‹ [Primeira resposta]
â—‹ [Segunda resposta]
â—‹ [Terceira resposta]
â—‹ [Quarta resposta]

ğŸ’¬ Ou descreva com suas prÃ³prias palavras.
```

ğŸš« FORMATO PROIBIDO (NÃƒO USE NUNCA):
```
âŒ ERRADO:
ğŸ’¡ Algumas sugestÃµes (responda livremente ou escolha uma):
â€¢ SugestÃ£o 1
â€¢ SugestÃ£o 2
```

âœ… EXEMPLO CORRETO:
```
â“ Pergunta {question_number}: Qual funcionalidade Ã© prioritÃ¡ria?

â—‹ Sistema de login e autenticaÃ§Ã£o
â—‹ Dashboard com relatÃ³rios
â—‹ IntegraÃ§Ã£o com pagamentos
â—‹ NotificaÃ§Ãµes por email

ğŸ’¬ Ou descreva com suas prÃ³prias palavras.
```

**REGRAS:**
1. Use APENAS "â—‹" (cÃ­rculo vazio) - NUNCA use "â€¢" ou "ğŸ’¡"
2. OpÃ§Ãµes sÃ£o RESPOSTAS diretas (nÃ£o perguntas!)
3. Exatamente 3-5 opÃ§Ãµes
4. Contextualize com respostas anteriores

**TÃ“PICOS A EXPLORAR (nÃ£o pergunte tudo, use bom senso):**

- VisÃ£o geral e objetivo do projeto
- Principais funcionalidades esperadas
- Quem sÃ£o os usuÃ¡rios
- Regras de negÃ³cio importantes
- IntegraÃ§Ãµes necessÃ¡rias
- Prioridades e MVP
- Requisitos tÃ©cnicos especiais

**QUANDO CONCLUIR:**

ApÃ³s 8-15 perguntas (ou quando tiver informaÃ§Ãµes suficientes), conclua a entrevista:
```
âœ… Obrigado! Coletei as informaÃ§Ãµes necessÃ¡rias para gerar o projeto.

Resumo do que entendi:
- [Ponto 1]
- [Ponto 2]
- [Ponto 3]

Vou gerar as tarefas do projeto agora.
```

**OUTPUT:** PortuguÃªs (Brasil). Continue com a prÃ³xima pergunta!
"""

    return system_prompt


async def handle_unified_open_interview(
    interview: Interview,
    project: Project,
    message_count: int,
    db: Session,
    parent_task: Optional[Task] = None
) -> Dict[str, Any]:
    """
    Handle unified open-ended interview.

    PROMPT #78 - Unified Open-Ended Interview System

    This replaces all fixed question handlers with a single AI-driven flow.
    All questions are generated by AI, open-ended, with optional suggestions.

    Args:
        interview: Interview instance
        project: Project instance
        message_count: Current message count
        db: Database session
        parent_task: Optional parent task for hierarchical interviews

    Returns:
        Response dict with success, message, and usage
    """
    logger.info(f"ğŸŒŸ UNIFIED OPEN-ENDED MODE - message_count={message_count}, interview_mode={interview.interview_mode}")

    # Extract previous answers from conversation
    previous_answers = {}
    for i, msg in enumerate(interview.conversation_data):
        if msg.get('role') == 'user':
            question_num = (i + 1) // 2
            previous_answers[f'q{question_num}'] = msg.get('content', '')

    # Build system prompt
    system_prompt = build_unified_open_prompt(
        project=project,
        interview=interview,
        message_count=message_count,
        parent_task=parent_task,
        previous_answers=previous_answers
    )

    # Retrieve previous questions from RAG for deduplication
    previous_questions_context = ""
    try:
        from app.services.rag_service import RAGService

        rag_service = RAGService(db)

        previous_questions = rag_service.retrieve(
            query="",
            filter={
                "type": "interview_question",
                "project_id": str(project.id)
            },
            top_k=30,
            similarity_threshold=0.0
        )

        if previous_questions:
            previous_questions_context = "\n\n**âš ï¸ PERGUNTAS JÃ FEITAS (NÃƒO REPITA):**\n"
            for i, pq in enumerate(previous_questions[:10], 1):
                previous_questions_context += f"{i}. {pq['content'][:80]}...\n"

            logger.info(f"âœ… RAG: Retrieved {len(previous_questions)} previous questions for deduplication")

    except Exception as e:
        logger.warning(f"âš ï¸  RAG retrieval failed: {e}")

    # Add previous questions to system prompt
    system_prompt += previous_questions_context

    # Prepare optimized messages for AI
    optimized_messages = prepare_interview_context(
        conversation_data=interview.conversation_data,
        max_recent=10
    )

    # Call AI Orchestrator
    orchestrator = AIOrchestrator(db)

    try:
        response = await orchestrator.execute(
            usage_type="interview",
            messages=optimized_messages,
            system_prompt=system_prompt,
            max_tokens=1000,
            project_id=interview.project_id,
            interview_id=interview.id
        )

        # Clean response
        cleaned_content = clean_ai_response(response["content"])

        # Parse for structured options (optional - for suggestion clicks)
        parsed_content, parsed_options = parse_ai_question_options(cleaned_content)

        # Build assistant message
        question_number = (message_count // 2) + 1

        # Determine question type based on parsed options
        if parsed_options:
            # Has options - use single_choice or multiple_choice from parser
            question_type = parsed_options.get("question_type", "single_choice")
            assistant_message = {
                "role": "assistant",
                "content": parsed_content,
                "timestamp": datetime.utcnow().isoformat(),
                "model": f"{response['provider']}/{response['model']}",
                "question_number": question_number,
                "question_type": question_type,
                "options": parsed_options.get("options", {}),
                "allow_custom_response": True  # PROMPT #79 - User can type freely OR click options
            }
        else:
            # No options - pure open-ended question
            assistant_message = {
                "role": "assistant",
                "content": parsed_content,
                "timestamp": datetime.utcnow().isoformat(),
                "model": f"{response['provider']}/{response['model']}",
                "question_number": question_number,
                "question_type": "text"  # Pure text input
            }

        # Append to conversation
        interview.conversation_data.append(assistant_message)
        flag_modified(interview, "conversation_data")
        interview.ai_model_used = response["model"]

        db.commit()
        db.refresh(interview)

        # Store question in RAG for deduplication
        try:
            deduplicator = InterviewQuestionDeduplicator(db)
            deduplicator.store_question(
                project_id=project.id,
                interview_id=interview.id,
                interview_mode=interview.interview_mode,
                question_text=parsed_content,
                question_number=question_number,
                is_fixed=False
            )
            logger.info(f"âœ… Stored Q{question_number} in RAG")
        except Exception as e:
            logger.error(f"âŒ Failed to store question in RAG: {e}")

        logger.info(f"âœ… AI responded successfully with open-ended question Q{question_number}")

        return {
            "success": True,
            "message": assistant_message,
            "usage": response.get("usage", {})
        }

    except Exception as ai_error:
        logger.error(f"âŒ AI execution failed: {str(ai_error)}", exc_info=True)

        # PROMPT #81 - Fallback: return a contextualized follow-up question
        question_number = (message_count // 2) + 1

        # Get last user response for context
        last_user_response = ""
        for msg in reversed(interview.conversation_data):
            if msg.get('role') == 'user':
                last_user_response = msg.get('content', '')[:100]
                break

        fallback_message = {
            "role": "assistant",
            "content": f"""ğŸ“‹ Continuando a entrevista para o projeto "{project.name}"...

â“ Pergunta {question_number}: Qual aspecto do projeto vocÃª gostaria de detalhar agora?

â—‹ Requisitos tÃ©cnicos e funcionais
â—‹ Perfil dos usuÃ¡rios e permissÃµes
â—‹ IntegraÃ§Ãµes com outros sistemas
â—‹ Cronograma e prioridades

ğŸ’¬ Ou descreva com suas prÃ³prias palavras.""",
            "timestamp": datetime.utcnow().isoformat(),
            "model": "system/fallback",
            "question_number": question_number,
            "question_type": "single_choice",
            "options": {
                "type": "single",
                "choices": [
                    {"id": "requisitos", "label": "Requisitos tÃ©cnicos e funcionais", "value": "requisitos"},
                    {"id": "usuarios", "label": "Perfil dos usuÃ¡rios e permissÃµes", "value": "usuarios"},
                    {"id": "integracoes", "label": "IntegraÃ§Ãµes com outros sistemas", "value": "integracoes"},
                    {"id": "cronograma", "label": "Cronograma e prioridades", "value": "cronograma"}
                ]
            },
            "allow_custom_response": True
        }

        # Save fallback message to conversation
        interview.conversation_data.append(fallback_message)
        flag_modified(interview, "conversation_data")
        interview.ai_model_used = "system/fallback"

        db.commit()
        db.refresh(interview)

        logger.warning(f"âš ï¸  Using fallback question Q{question_number} for interview {interview.id}")

        return {
            "success": True,
            "message": fallback_message,
            "usage": {"fallback": True, "error": str(ai_error)}
        }


async def generate_first_question(
    interview: Interview,
    project: Project,
    db: Session,
    parent_task: Optional[Task] = None
) -> Dict[str, Any]:
    """
    Generate the first open-ended question for an interview.

    PROMPT #78 - Unified Open-Ended Interview System

    This replaces the fixed Q1 (Title) with an AI-generated open question.

    Args:
        interview: Interview instance
        project: Project instance
        db: Database session
        parent_task: Optional parent task for hierarchical interviews

    Returns:
        First question message dict
    """
    logger.info(f"ğŸŒŸ Generating FIRST open-ended question for interview {interview.id}")

    # Build context for first question
    parent_context = ""
    if parent_task:
        parent_context = f"""
VocÃª estÃ¡ criando um item dentro de "{parent_task.title}" ({parent_task.item_type}).
Contextualize sua primeira pergunta com base no card pai.
"""

    # PROMPT #81 - EXTREMELY explicit prompt for CLOSED questions
    first_question_prompt = f"""ğŸš¨ ATENÃ‡ÃƒO: Siga o formato EXATAMENTE como especificado abaixo! ğŸš¨

VocÃª Ã© um Product Owner iniciando uma entrevista para coletar requisitos.

**PROJETO:** {project.name or 'Novo Projeto'}
**DESCRIÃ‡ÃƒO:** {project.description or 'NÃ£o definida'}
{parent_context}

âš ï¸ IMPORTANTE: Use APENAS o sÃ­mbolo "â—‹" para as opÃ§Ãµes!
âš ï¸ PROIBIDO usar "â€¢" ou "ğŸ’¡ Algumas sugestÃµes"!

**FORMATO OBRIGATÃ“RIO (copie exatamente):**
```
ğŸ‘‹ OlÃ¡! Vou ajudar a definir os requisitos do seu projeto "{project.name or 'Novo Projeto'}".

â“ Pergunta 1: [FaÃ§a uma pergunta FECHADA aqui]

â—‹ [Primeira opÃ§Ã£o de resposta]
â—‹ [Segunda opÃ§Ã£o de resposta]
â—‹ [Terceira opÃ§Ã£o de resposta]
â—‹ [Quarta opÃ§Ã£o de resposta]

ğŸ’¬ Ou descreva com suas prÃ³prias palavras.
```

ğŸš« FORMATO PROIBIDO (NÃƒO USE):
```
âŒ ERRADO:
ğŸ’¡ Algumas sugestÃµes (responda livremente ou escolha uma):
â€¢ OpÃ§Ã£o 1
â€¢ OpÃ§Ã£o 2
```

âœ… FORMATO CORRETO:
```
â—‹ Automatizar processos manuais da empresa
â—‹ Criar uma plataforma digital de vendas
â—‹ Integrar sistemas existentes
â—‹ Melhorar experiÃªncia do cliente
```

**REGRAS:**
1. Use APENAS "â—‹" (cÃ­rculo vazio) para opÃ§Ãµes
2. OpÃ§Ãµes sÃ£o RESPOSTAS diretas, nÃ£o perguntas
3. Exatamente 3-5 opÃ§Ãµes
4. Pergunta deve ser FECHADA

Gere a pergunta agora usando o FORMATO OBRIGATÃ“RIO com "â—‹"!
"""

    # Call AI Orchestrator
    orchestrator = AIOrchestrator(db)

    try:
        # PROMPT #81 - API requires at least one message
        initial_messages = [
            {"role": "user", "content": "Comece a entrevista para coletar requisitos do projeto."}
        ]

        response = await orchestrator.execute(
            usage_type="interview",
            messages=initial_messages,
            system_prompt=first_question_prompt,
            max_tokens=500,
            project_id=interview.project_id,
            interview_id=interview.id
        )

        # Clean response
        cleaned_content = clean_ai_response(response["content"])

        # Parse for suggestions
        parsed_content, parsed_options = parse_ai_question_options(cleaned_content)

        # Build assistant message with proper question type
        if parsed_options:
            # Has options - use single_choice or multiple_choice from parser
            question_type = parsed_options.get("question_type", "single_choice")
            assistant_message = {
                "role": "assistant",
                "content": parsed_content,
                "timestamp": datetime.utcnow().isoformat(),
                "model": f"{response['provider']}/{response['model']}",
                "question_number": 1,
                "question_type": question_type,
                "options": parsed_options.get("options", {}),
                "allow_custom_response": True  # PROMPT #79 - User can type freely OR click options
            }
        else:
            # No options - pure text input
            assistant_message = {
                "role": "assistant",
                "content": parsed_content,
                "timestamp": datetime.utcnow().isoformat(),
                "model": f"{response['provider']}/{response['model']}",
                "question_number": 1,
                "question_type": "text"  # Pure text input
            }

        logger.info(f"âœ… First open-ended question generated successfully")

        return assistant_message

    except Exception as ai_error:
        logger.error(f"âŒ Failed to generate first question: {str(ai_error)}", exc_info=True)

        # PROMPT #81 - Fallback: return a contextualized first question with error info
        fallback_message = {
            "role": "assistant",
            "content": f"""ğŸ‘‹ OlÃ¡! Vou ajudar a refinar os requisitos do projeto "{project.name}".

ğŸ“‹ VocÃª descreveu: "{project.description}"

â“ Pergunta 1: Com base nisso, qual seria a primeira funcionalidade principal que vocÃª precisa implementar?

â—‹ Sistema de autenticaÃ§Ã£o e controle de acesso
â—‹ Interface para gerenciamento de dados
â—‹ IntegraÃ§Ã£o com sistemas externos
â—‹ Processamento e anÃ¡lise de informaÃ§Ãµes

ğŸ’¬ Ou descreva com suas prÃ³prias palavras.""",
            "timestamp": datetime.utcnow().isoformat(),
            "model": "system/fallback",
            "question_number": 1,
            "question_type": "single_choice",
            "options": {
                "type": "single",
                "choices": [
                    {"id": "autenticacao", "label": "Sistema de autenticaÃ§Ã£o e controle de acesso", "value": "autenticacao"},
                    {"id": "gerenciamento_dados", "label": "Interface para gerenciamento de dados", "value": "gerenciamento_dados"},
                    {"id": "integracao", "label": "IntegraÃ§Ã£o com sistemas externos", "value": "integracao"},
                    {"id": "processamento", "label": "Processamento e anÃ¡lise de informaÃ§Ãµes", "value": "processamento"}
                ]
            },
            "allow_custom_response": True,  # PROMPT #79 - User can type freely OR click options
            "fallback_error": str(ai_error)  # PROMPT #81 - Include error for UI display
        }

        return fallback_message
