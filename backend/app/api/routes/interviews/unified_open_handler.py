"""
Unified Open-Ended Interview Handler
PROMPT #78 - Unified Open-Ended Interview System

All interviews now use the same open-ended question format:
- Questions are open-ended (like GPT), not fixed
- AI generates questions freely based on context
- Response options are SUGGESTIONS, not requirements
- User can respond freely with text or click suggestions

This replaces all the fixed question modes (meta_prompt, requirements, task_focused, etc.)
"""

from typing import Dict, Any, Optional
from uuid import UUID
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy.orm.attributes import flag_modified
from fastapi import HTTPException, status
import logging

from app.models.interview import Interview
from app.models.project import Project
from app.models.task import Task
from app.services.ai_orchestrator import AIOrchestrator
from app.services.interview_question_deduplicator import InterviewQuestionDeduplicator
from app.api.routes.interviews.option_parser import parse_ai_question_options
from app.api.routes.interviews.response_cleaners import clean_ai_response
from app.api.routes.interviews.context_builders import prepare_interview_context

logger = logging.getLogger(__name__)


def build_unified_open_prompt(
    project: Project,
    interview: Interview,
    message_count: int,
    parent_task: Optional[Task] = None,
    previous_answers: Optional[Dict] = None
) -> str:
    """
    Build the system prompt for unified open-ended interviews.

    PROMPT #78 - Unified Open-Ended Interview System

    Key principles:
    1. Questions are OPEN-ENDED (like GPT)
    2. User can respond FREELY
    3. AI can offer SUGGESTIONS (optional)
    4. No fixed questions - AI decides what to ask

    Args:
        project: Project instance
        interview: Interview instance
        message_count: Current message count
        parent_task: Optional parent task for hierarchical interviews
        previous_answers: Dict of previous answers

    Returns:
        System prompt string
    """
    previous_answers = previous_answers or {}
    question_number = (message_count // 2) + 1

    # Build project context
    project_context = f"""
**PROJETO:**
- Nome: {project.name or 'NÃ£o definido'}
- DescriÃ§Ã£o: {project.description or 'NÃ£o definida'}
"""

    # Add stack context if available
    if project.stack_backend:
        project_context += f"""
**STACK TÃ‰CNICA:**
- Backend: {project.stack_backend}
- Database: {project.stack_database or 'NÃ£o definido'}
- Frontend: {project.stack_frontend or 'NÃ£o definido'}
- CSS: {project.stack_css or 'NÃ£o definido'}
- Mobile: {project.stack_mobile or 'NÃ£o definido'}
"""

    # Add parent task context for hierarchical interviews
    parent_context = ""
    if parent_task:
        parent_context = f"""
**CARD PAI:**
- Tipo: {parent_task.item_type or 'Task'}
- TÃ­tulo: {parent_task.title}
- DescriÃ§Ã£o: {parent_task.description or 'NÃ£o definida'}
"""

    # Build the unified open-ended prompt
    system_prompt = f"""VocÃª Ã© um Product Owner experiente conduzindo uma entrevista para coletar requisitos de software.

{project_context}
{parent_context}

**ESTILO DE ENTREVISTA - PERGUNTAS ABERTAS (PROMPT #78):**

VocÃª deve fazer perguntas ABERTAS, como um assistente GPT. O usuÃ¡rio tem liberdade total para responder.

**REGRAS IMPORTANTES:**

1. âœ… **PERGUNTAS ABERTAS** - FaÃ§a perguntas abertas que permitam respostas livres
2. âœ… **SUGESTÃ•ES OPCIONAIS** - VocÃª PODE oferecer sugestÃµes de resposta, mas sÃ£o OPCIONAIS
3. âœ… **ACEITAR QUALQUER RESPOSTA** - O usuÃ¡rio pode responder livremente com texto
4. âœ… **CONTEXTO** - Use as respostas anteriores para fazer perguntas contextualizadas
5. âœ… **PROGRESSO NATURAL** - Avance naturalmente pelos tÃ³picos relevantes
6. âœ… **UMA PERGUNTA POR VEZ** - FaÃ§a apenas uma pergunta por mensagem

**FORMATO DAS PERGUNTAS:**

Para perguntas abertas simples:
```
â“ Pergunta {question_number}: [Sua pergunta aqui]

ğŸ’¬ Responda livremente.
```

Para perguntas com sugestÃµes (OPCIONAL):
```
â“ Pergunta {question_number}: [Sua pergunta aqui]

ğŸ’¡ Algumas sugestÃµes (responda livremente ou escolha uma):
â€¢ SugestÃ£o 1
â€¢ SugestÃ£o 2
â€¢ SugestÃ£o 3

ğŸ’¬ Ou descreva com suas prÃ³prias palavras.
```

**EXEMPLOS CORRETOS:**

âœ… **Pergunta aberta simples:**
â“ Pergunta 1: O que vocÃª espera que este sistema faÃ§a?

ğŸ’¬ Responda livremente.

âœ… **Pergunta com sugestÃµes opcionais:**
â“ Pergunta 2: Qual Ã© o principal problema que vocÃª quer resolver?

ğŸ’¡ Algumas sugestÃµes (responda livremente ou escolha uma):
â€¢ Automatizar processos manuais
â€¢ Melhorar a experiÃªncia do usuÃ¡rio
â€¢ Reduzir custos operacionais
â€¢ Aumentar vendas

ğŸ’¬ Ou descreva com suas prÃ³prias palavras.

**TÃ“PICOS A EXPLORAR (nÃ£o pergunte tudo, use bom senso):**

- VisÃ£o geral e objetivo do projeto
- Principais funcionalidades esperadas
- Quem sÃ£o os usuÃ¡rios
- Regras de negÃ³cio importantes
- IntegraÃ§Ãµes necessÃ¡rias
- Prioridades e MVP
- Requisitos tÃ©cnicos especiais

**QUANDO CONCLUIR:**

ApÃ³s 8-15 perguntas (ou quando tiver informaÃ§Ãµes suficientes), conclua a entrevista:
```
âœ… Obrigado! Coletei as informaÃ§Ãµes necessÃ¡rias para gerar o projeto.

Resumo do que entendi:
- [Ponto 1]
- [Ponto 2]
- [Ponto 3]

Vou gerar as tarefas do projeto agora.
```

**OUTPUT:** PortuguÃªs (Brasil). Continue com a prÃ³xima pergunta!
"""

    return system_prompt


async def handle_unified_open_interview(
    interview: Interview,
    project: Project,
    message_count: int,
    db: Session,
    parent_task: Optional[Task] = None
) -> Dict[str, Any]:
    """
    Handle unified open-ended interview.

    PROMPT #78 - Unified Open-Ended Interview System

    This replaces all fixed question handlers with a single AI-driven flow.
    All questions are generated by AI, open-ended, with optional suggestions.

    Args:
        interview: Interview instance
        project: Project instance
        message_count: Current message count
        db: Database session
        parent_task: Optional parent task for hierarchical interviews

    Returns:
        Response dict with success, message, and usage
    """
    logger.info(f"ğŸŒŸ UNIFIED OPEN-ENDED MODE - message_count={message_count}, interview_mode={interview.interview_mode}")

    # Extract previous answers from conversation
    previous_answers = {}
    for i, msg in enumerate(interview.conversation_data):
        if msg.get('role') == 'user':
            question_num = (i + 1) // 2
            previous_answers[f'q{question_num}'] = msg.get('content', '')

    # Build system prompt
    system_prompt = build_unified_open_prompt(
        project=project,
        interview=interview,
        message_count=message_count,
        parent_task=parent_task,
        previous_answers=previous_answers
    )

    # Retrieve previous questions from RAG for deduplication
    previous_questions_context = ""
    try:
        from app.services.rag_service import RAGService

        rag_service = RAGService(db)

        previous_questions = rag_service.retrieve(
            query="",
            filter={
                "type": "interview_question",
                "project_id": str(project.id)
            },
            top_k=30,
            similarity_threshold=0.0
        )

        if previous_questions:
            previous_questions_context = "\n\n**âš ï¸ PERGUNTAS JÃ FEITAS (NÃƒO REPITA):**\n"
            for i, pq in enumerate(previous_questions[:10], 1):
                previous_questions_context += f"{i}. {pq['content'][:80]}...\n"

            logger.info(f"âœ… RAG: Retrieved {len(previous_questions)} previous questions for deduplication")

    except Exception as e:
        logger.warning(f"âš ï¸  RAG retrieval failed: {e}")

    # Add previous questions to system prompt
    system_prompt += previous_questions_context

    # Prepare optimized messages for AI
    optimized_messages = prepare_interview_context(
        conversation_data=interview.conversation_data,
        max_recent=10
    )

    # Call AI Orchestrator
    orchestrator = AIOrchestrator(db)

    try:
        response = await orchestrator.execute(
            usage_type="interview",
            messages=optimized_messages,
            system_prompt=system_prompt,
            max_tokens=1000,
            project_id=interview.project_id,
            interview_id=interview.id
        )

        # Clean response
        cleaned_content = clean_ai_response(response["content"])

        # Parse for structured options (optional - for suggestion clicks)
        parsed_content, parsed_options = parse_ai_question_options(cleaned_content)

        # Build assistant message
        question_number = (message_count // 2) + 1
        assistant_message = {
            "role": "assistant",
            "content": parsed_content,
            "timestamp": datetime.utcnow().isoformat(),
            "model": f"{response['provider']}/{response['model']}",
            "question_number": question_number,
            "question_type": "open_ended"  # Always open-ended now
        }

        # Add suggestions if parsed (for frontend rendering)
        if parsed_options:
            assistant_message["suggestions"] = parsed_options.get("options", {}).get("choices", [])

        # Append to conversation
        interview.conversation_data.append(assistant_message)
        flag_modified(interview, "conversation_data")
        interview.ai_model_used = response["model"]

        db.commit()
        db.refresh(interview)

        # Store question in RAG for deduplication
        try:
            deduplicator = InterviewQuestionDeduplicator(db)
            deduplicator.store_question(
                project_id=project.id,
                interview_id=interview.id,
                interview_mode=interview.interview_mode,
                question_text=parsed_content,
                question_number=question_number,
                is_fixed=False
            )
            logger.info(f"âœ… Stored Q{question_number} in RAG")
        except Exception as e:
            logger.error(f"âŒ Failed to store question in RAG: {e}")

        logger.info(f"âœ… AI responded successfully with open-ended question Q{question_number}")

        return {
            "success": True,
            "message": assistant_message,
            "usage": response.get("usage", {})
        }

    except Exception as ai_error:
        logger.error(f"âŒ AI execution failed: {str(ai_error)}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get AI response: {str(ai_error)}"
        )


async def generate_first_question(
    interview: Interview,
    project: Project,
    db: Session,
    parent_task: Optional[Task] = None
) -> Dict[str, Any]:
    """
    Generate the first open-ended question for an interview.

    PROMPT #78 - Unified Open-Ended Interview System

    This replaces the fixed Q1 (Title) with an AI-generated open question.

    Args:
        interview: Interview instance
        project: Project instance
        db: Database session
        parent_task: Optional parent task for hierarchical interviews

    Returns:
        First question message dict
    """
    logger.info(f"ğŸŒŸ Generating FIRST open-ended question for interview {interview.id}")

    # Build context for first question
    parent_context = ""
    if parent_task:
        parent_context = f"""
VocÃª estÃ¡ criando um item dentro de "{parent_task.title}" ({parent_task.item_type}).
Contextualize sua primeira pergunta com base no card pai.
"""

    # Simple prompt for first question
    first_question_prompt = f"""VocÃª Ã© um Product Owner iniciando uma entrevista para coletar requisitos.

**PROJETO:** {project.name or 'Novo Projeto'}
**DESCRIÃ‡ÃƒO:** {project.description or 'NÃ£o definida'}
{parent_context}

**TAREFA:** FaÃ§a a PRIMEIRA pergunta da entrevista.

**REGRAS:**
1. Pergunta ABERTA (o usuÃ¡rio responde livremente)
2. Pode oferecer SUGESTÃ•ES opcionais
3. Seja amigÃ¡vel e acolhedor
4. Use portuguÃªs brasileiro

**FORMATO:**
```
ğŸ‘‹ OlÃ¡! Vou ajudar a definir os requisitos do seu projeto.

â“ Pergunta 1: [Sua pergunta inicial aqui]

ğŸ’¡ Algumas sugestÃµes (responda livremente ou escolha uma):
â€¢ SugestÃ£o 1
â€¢ SugestÃ£o 2
â€¢ SugestÃ£o 3

ğŸ’¬ Ou descreva com suas prÃ³prias palavras.
```

Gere a primeira pergunta agora!
"""

    # Call AI Orchestrator
    orchestrator = AIOrchestrator(db)

    try:
        response = await orchestrator.execute(
            usage_type="interview",
            messages=[],  # No previous messages
            system_prompt=first_question_prompt,
            max_tokens=500,
            project_id=interview.project_id,
            interview_id=interview.id
        )

        # Clean response
        cleaned_content = clean_ai_response(response["content"])

        # Parse for suggestions
        parsed_content, parsed_options = parse_ai_question_options(cleaned_content)

        # Build assistant message
        assistant_message = {
            "role": "assistant",
            "content": parsed_content,
            "timestamp": datetime.utcnow().isoformat(),
            "model": f"{response['provider']}/{response['model']}",
            "question_number": 1,
            "question_type": "open_ended"
        }

        # Add suggestions if parsed
        if parsed_options:
            assistant_message["suggestions"] = parsed_options.get("options", {}).get("choices", [])

        logger.info(f"âœ… First open-ended question generated successfully")

        return assistant_message

    except Exception as ai_error:
        logger.error(f"âŒ Failed to generate first question: {str(ai_error)}", exc_info=True)

        # Fallback: return a simple first question
        return {
            "role": "assistant",
            "content": """ğŸ‘‹ OlÃ¡! Vou ajudar a definir os requisitos do seu projeto.

â“ Pergunta 1: O que vocÃª espera que este sistema faÃ§a?

ğŸ’¡ Algumas sugestÃµes (responda livremente ou escolha uma):
â€¢ Automatizar processos manuais
â€¢ Gerenciar dados e informaÃ§Ãµes
â€¢ Conectar usuÃ¡rios e serviÃ§os
â€¢ Melhorar a experiÃªncia do cliente

ğŸ’¬ Ou descreva com suas prÃ³prias palavras.""",
            "timestamp": datetime.utcnow().isoformat(),
            "model": "system/fallback",
            "question_number": 1,
            "question_type": "open_ended"
        }
