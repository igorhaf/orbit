"""
ContextGeneratorService
PROMPT #89 - Generate project context from Context Interview
PROMPT #92 - Generate suggested epics from context
PROMPT #94 - Activate/Reject suggested epics

This service processes the Context Interview and generates:
- context_semantic: Structured semantic text for AI consumption
- context_human: Human-readable project description
- suggested_epics: List of macro-level epics covering all project modules

The context is the foundational, immutable description of the project
that guides all subsequent interviews and card generation.
"""

from typing import Dict, List, Optional
from uuid import UUID, uuid4
from datetime import datetime
from sqlalchemy.orm import Session
import json
import logging
import re

from app.models.project import Project
from app.models.interview import Interview, InterviewStatus
from app.models.task import Task, TaskStatus, ItemType, PriorityLevel
from app.services.ai_orchestrator import AIOrchestrator
from app.prompter.facade import PrompterFacade

logger = logging.getLogger(__name__)


def _strip_markdown_json(content: str) -> str:
    """
    Remove markdown code blocks from JSON response.
    AI sometimes returns JSON wrapped in ```json ... ``` blocks.
    """
    content = re.sub(r'^```json\s*\n?', '', content, flags=re.MULTILINE)
    content = re.sub(r'\n?```\s*$', '', content, flags=re.MULTILINE)
    return content.strip()


def _convert_semantic_to_human(semantic_text: str, semantic_map: Dict[str, str]) -> str:
    """
    PROMPT #89 - Convert semantic text to human-readable text.

    This function transforms semantic references (identifiers like N1, P1, etc.)
    into their actual meanings, creating natural prose.

    Args:
        semantic_text: Text with semantic identifiers
        semantic_map: Dictionary mapping identifiers to meanings

    Returns:
        Human-readable text with identifiers replaced
    """
    if not semantic_map or not semantic_text:
        return semantic_text or ""

    human_text = semantic_text

    # Sort identifiers by length (longest first) to avoid partial replacements
    sorted_identifiers = sorted(semantic_map.keys(), key=len, reverse=True)

    for identifier in sorted_identifiers:
        meaning = semantic_map[identifier]
        pattern = rf'\b{re.escape(identifier)}\b'
        human_text = re.sub(pattern, meaning, human_text)

    # Clean up multiple consecutive newlines
    human_text = re.sub(r'\n{3,}', '\n\n', human_text)

    return human_text.strip()


class ContextGeneratorService:
    """
    Service for generating project context from Context Interview.

    PROMPT #89 - Context Interview: Foundational Project Description

    This service:
    1. Analyzes the Context Interview conversation
    2. Generates structured semantic text (for AI)
    3. Converts to human-readable description
    4. Saves both to the Project model
    """

    def __init__(self, db: Session):
        self.db = db
        try:
            self.prompter = PrompterFacade(db)
        except RuntimeError:
            self.prompter = None
        self.orchestrator = AIOrchestrator(db)

    async def generate_context_from_interview(
        self,
        interview_id: UUID,
        project_id: UUID
    ) -> Dict:
        """
        Generate project context from Context Interview conversation.

        PROMPT #89 - Context Interview Processing

        Flow:
        1. Validate interview (must be context mode, have enough messages)
        2. AI analyzes conversation and generates structured context
        3. Extract semantic map and create human-readable version
        4. Save to Project model
        5. Mark interview as completed

        Args:
            interview_id: Context Interview ID
            project_id: Project ID

        Returns:
            {
                "context_semantic": str,
                "context_human": str,
                "semantic_map": Dict[str, str],
                "interview_insights": {
                    "project_vision": str,
                    "problem_statement": str,
                    "key_features": [str, ...],
                    "target_users": [str, ...],
                    "success_criteria": [str, ...]
                }
            }

        Raises:
            ValueError: If interview not found, wrong mode, or insufficient data
        """
        # 1. Validate interview
        interview = self.db.query(Interview).filter(Interview.id == interview_id).first()
        if not interview:
            raise ValueError(f"Interview {interview_id} not found")

        # Accept both "context" and "meta_prompt" modes for compatibility
        if interview.interview_mode not in ["context", "meta_prompt"]:
            raise ValueError(
                f"Interview {interview_id} is not a context interview "
                f"(mode: {interview.interview_mode}). Only 'context' mode supported."
            )

        # Minimum 6 messages (3 Q&A pairs - Q1, Q2, Q3 at least)
        if not interview.conversation_data or len(interview.conversation_data) < 6:
            raise ValueError(
                f"Interview {interview_id} has insufficient data. "
                f"Need at least 6 messages (3 Q&A pairs), got {len(interview.conversation_data or [])}."
            )

        # 2. Get project
        project = self.db.query(Project).filter(Project.id == project_id).first()
        if not project:
            raise ValueError(f"Project {project_id} not found")

        # Check if context is already locked
        if project.context_locked:
            raise ValueError(
                f"Project {project_id} context is already locked. "
                "Cannot regenerate context after first epic is created."
            )

        # 3. Build conversation summary for AI
        conversation_summary = self._build_conversation_summary(interview.conversation_data)

        # 4. Generate context using AI
        context_result = await self._generate_context_with_ai(
            project=project,
            conversation_summary=conversation_summary
        )

        # 5. Save to project
        project.context_semantic = context_result["context_semantic"]
        project.context_human = context_result["context_human"]
        project.description = context_result["context_human"]  # Also update description

        # 6. Mark interview as completed
        interview.status = InterviewStatus.COMPLETED

        self.db.commit()

        logger.info(f"‚úÖ Context generated for project {project.name}")
        logger.info(f"   - Semantic: {len(context_result['context_semantic'])} chars")
        logger.info(f"   - Human: {len(context_result['context_human'])} chars")

        # 7. PROMPT #92 - Generate suggested epics from context
        try:
            suggested_epics = await self.generate_suggested_epics(
                project_id=project_id,
                context_human=context_result["context_human"],
                interview_insights=context_result.get("interview_insights", {})
            )
            context_result["suggested_epics"] = suggested_epics
            logger.info(f"   - Suggested Epics: {len(suggested_epics)}")
        except Exception as e:
            logger.error(f"Failed to generate suggested epics: {e}")
            context_result["suggested_epics"] = []

        return context_result

    def _build_conversation_summary(self, conversation_data: List[Dict]) -> str:
        """
        Build a structured summary of the conversation for AI processing.

        Args:
            conversation_data: List of conversation messages

        Returns:
            Formatted conversation summary
        """
        summary_parts = []

        for i, msg in enumerate(conversation_data):
            role = msg.get("role", "unknown")
            content = msg.get("content", "")

            if role == "assistant":
                # AI question
                summary_parts.append(f"**Pergunta:** {content}")
            elif role == "user":
                # User answer
                summary_parts.append(f"**Resposta:** {content}")
                summary_parts.append("")  # Empty line between Q&A pairs

        return "\n".join(summary_parts)

    async def _generate_context_with_ai(
        self,
        project: Project,
        conversation_summary: str
    ) -> Dict:
        """
        Use AI to generate structured context from conversation.

        Args:
            project: Project instance
            conversation_summary: Formatted conversation summary

        Returns:
            Dict with context_semantic, context_human, semantic_map, and insights
        """
        system_prompt = """Voc√™ √© um especialista em an√°lise de requisitos de software.

Sua tarefa √© analisar uma entrevista de contexto de projeto e gerar:

1. **CONTEXTO SEM√ÇNTICO** (context_semantic):
   - Texto estruturado com identificadores sem√¢nticos
   - Use identificadores como: N1 (nome), P1 (problema), V1 (vis√£o), U1 (usu√°rio), F1 (funcionalidade)
   - Inclua um Mapa Sem√¢ntico no final com todas as defini√ß√µes

2. **MAPA SEM√ÇNTICO** (semantic_map):
   - Dicion√°rio JSON mapeando cada identificador para seu significado
   - Exemplo: {"N1": "Sistema de Vendas", "P1": "Gest√£o de estoque ineficiente"}

3. **INSIGHTS DA ENTREVISTA** (interview_insights):
   - project_vision: Vis√£o geral do projeto
   - problem_statement: Problema que o projeto resolve
   - key_features: Lista de funcionalidades principais
   - target_users: Tipos de usu√°rios do sistema
   - success_criteria: Crit√©rios de sucesso

FORMATO DE RESPOSTA (JSON):
```json
{
    "context_semantic": "## Contexto do Projeto\\n\\n### Vis√£o\\nN1 √© um sistema que resolve P1...\\n\\n### Usu√°rios\\n- U1: ...\\n\\n## Mapa Sem√¢ntico\\n- **N1**: Nome do projeto\\n- **P1**: Problema principal",
    "semantic_map": {
        "N1": "Nome do Projeto",
        "P1": "Problema principal",
        "V1": "Vis√£o do projeto",
        "U1": "Primeiro tipo de usu√°rio",
        "F1": "Primeira funcionalidade"
    },
    "interview_insights": {
        "project_vision": "Desenvolver um sistema...",
        "problem_statement": "Atualmente o cliente enfrenta...",
        "key_features": ["Feature 1", "Feature 2"],
        "target_users": ["Admin", "Usu√°rio Final"],
        "success_criteria": ["Reduzir tempo de...", "Aumentar efici√™ncia..."]
    }
}
```

IMPORTANTE:
- O context_semantic deve ser rico e detalhado (m√≠nimo 500 caracteres)
- Use portugu√™s brasileiro
- Os identificadores devem ser concisos (2-3 caracteres)
- O Mapa Sem√¢ntico deve estar DENTRO do context_semantic no final
- Retorne APENAS o JSON, sem texto adicional"""

        user_prompt = f"""Analise a seguinte entrevista de contexto para o projeto "{project.name}":

{conversation_summary}

Gere o contexto sem√¢ntico estruturado, o mapa sem√¢ntico e os insights conforme especificado."""

        # Call AI
        messages = [{"role": "user", "content": user_prompt}]

        response = await self.orchestrator.execute(
            usage_type="prompt_generation",
            messages=messages,
            system_prompt=system_prompt,
            max_tokens=4000
            # Note: temperature is configured in the AI model settings in the database
        )

        # Parse response
        response_text = response.get("content", "")
        response_text = _strip_markdown_json(response_text)

        try:
            result = json.loads(response_text)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse AI response as JSON: {e}")
            logger.error(f"Response text: {response_text[:500]}...")
            raise ValueError("AI response was not valid JSON. Please try again.")

        # Validate required fields
        if "context_semantic" not in result:
            raise ValueError("AI response missing 'context_semantic' field")

        semantic_map = result.get("semantic_map", {})
        context_semantic = result["context_semantic"]

        # Convert semantic to human-readable
        context_human = _convert_semantic_to_human(context_semantic, semantic_map)

        # Remove the Mapa Sem√¢ntico section from human text
        context_human = re.sub(
            r'##\s*Mapa\s*Sem[a√¢]ntico\s*\n+(?:[-*]\s*\*\*[^*]+\*\*:[^\n]*\n*)*',
            '',
            context_human,
            flags=re.IGNORECASE | re.MULTILINE
        )
        context_human = context_human.strip()

        return {
            "context_semantic": context_semantic,
            "context_human": context_human,
            "semantic_map": semantic_map,
            "interview_insights": result.get("interview_insights", {})
        }

    async def generate_suggested_epics(
        self,
        project_id: UUID,
        context_human: str,
        interview_insights: Dict
    ) -> List[Dict]:
        """
        PROMPT #92 - Generate suggested epics from project context.

        Generates a comprehensive list of macro-level epics (modules) that
        cover the entire scope of the project based on the context interview.

        All epics are created as suggestions (inactive) with labels=["suggested"].
        They appear grayed out in the UI until the user activates them.

        Args:
            project_id: Project ID
            context_human: Human-readable project context
            interview_insights: Insights extracted from the context interview

        Returns:
            List of suggested epic dictionaries
        """
        system_prompt = """Voc√™ √© um arquiteto de software especialista em decomposi√ß√£o de sistemas.

Sua tarefa √© analisar o contexto de um projeto e gerar uma lista ABRANGENTE de √âpicos (m√≥dulos macro) que cubram TODO o escopo do sistema.

REGRAS:
1. Cada √©pico representa um M√ìDULO ou √ÅREA FUNCIONAL macro do sistema
2. A lista deve ser COMPLETA - cobrir 100% das funcionalidades mencionadas no contexto
3. Pense em termos de m√≥dulos de software (Autentica√ß√£o, Dashboard, Relat√≥rios, Configura√ß√µes, etc.)
4. Inclua tamb√©m √©picos de infraestrutura se relevante (Setup Inicial, Deploy, Integra√ß√µes)
5. Use nomes CURTOS e DESCRITIVOS para os √©picos (m√°x 50 caracteres)
6. A descri√ß√£o deve ser breve (1-2 frases) explicando o escopo do m√≥dulo
7. Ordene por prioridade/depend√™ncia l√≥gica (fundacionais primeiro)

FORMATO DE RESPOSTA (JSON):
```json
{
    "epics": [
        {
            "title": "Autentica√ß√£o e Autoriza√ß√£o",
            "description": "Sistema de login, registro, recupera√ß√£o de senha e controle de permiss√µes por perfil.",
            "priority": "critical",
            "order": 1
        },
        {
            "title": "Dashboard Principal",
            "description": "Tela inicial com indicadores chave, resumos e acesso r√°pido √†s principais funcionalidades.",
            "priority": "high",
            "order": 2
        }
    ]
}
```

PRIORIDADES V√ÅLIDAS: critical, high, medium, low

IMPORTANTE:
- Gere entre 8 e 20 √©picos dependendo da complexidade do projeto
- Cubra TODAS as √°reas mencionadas no contexto
- Inclua √©picos impl√≠citos (toda aplica√ß√£o precisa de autentica√ß√£o, configura√ß√µes, etc.)
- Retorne APENAS o JSON, sem texto adicional"""

        # Build user prompt with context
        key_features = interview_insights.get("key_features", [])
        target_users = interview_insights.get("target_users", [])

        features_text = "\n".join([f"- {f}" for f in key_features]) if key_features else "N√£o especificadas"
        users_text = "\n".join([f"- {u}" for u in target_users]) if target_users else "N√£o especificados"

        user_prompt = f"""Analise o seguinte contexto de projeto e gere a lista completa de √âpicos:

## CONTEXTO DO PROJETO
{context_human}

## FUNCIONALIDADES IDENTIFICADAS
{features_text}

## USU√ÅRIOS DO SISTEMA
{users_text}

Gere a lista de √âpicos (m√≥dulos macro) que cubra 100% do escopo deste projeto."""

        # Call AI
        messages = [{"role": "user", "content": user_prompt}]

        response = await self.orchestrator.execute(
            usage_type="prompt_generation",
            messages=messages,
            system_prompt=system_prompt,
            max_tokens=4000
        )

        # Parse response
        response_text = response.get("content", "")
        response_text = _strip_markdown_json(response_text)

        try:
            result = json.loads(response_text)
            epics = result.get("epics", [])
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse epic suggestions as JSON: {e}")
            logger.error(f"Response text: {response_text[:500]}...")
            # Return empty list on error - don't fail the whole process
            return []

        # Save epics to database
        saved_epics = []
        priority_map = {
            "critical": PriorityLevel.CRITICAL,
            "high": PriorityLevel.HIGH,
            "medium": PriorityLevel.MEDIUM,
            "low": PriorityLevel.LOW
        }

        for i, epic_data in enumerate(epics):
            try:
                epic = Task(
                    id=uuid4(),
                    project_id=project_id,
                    title=epic_data.get("title", f"√âpico {i+1}")[:255],
                    description=epic_data.get("description", ""),
                    item_type=ItemType.EPIC,
                    status=TaskStatus.BACKLOG,
                    priority=priority_map.get(epic_data.get("priority", "medium"), PriorityLevel.MEDIUM),
                    order=epic_data.get("order", i + 1),
                    labels=["suggested"],  # Mark as suggested (inactive)
                    workflow_state="draft",  # Draft state for suggested items
                    reporter="system",
                    created_at=datetime.utcnow(),
                    updated_at=datetime.utcnow()
                )
                self.db.add(epic)
                saved_epics.append({
                    "id": str(epic.id),
                    "title": epic.title,
                    "description": epic.description,
                    "priority": epic_data.get("priority", "medium"),
                    "order": epic.order
                })
            except Exception as e:
                logger.error(f"Failed to create epic '{epic_data.get('title')}': {e}")
                continue

        self.db.commit()

        logger.info(f"‚úÖ Generated {len(saved_epics)} suggested epics for project {project_id}")

        return saved_epics

    async def lock_context(self, project_id: UUID) -> bool:
        """
        Lock the project context, making it immutable.

        PROMPT #89 - Context is locked automatically when first epic is generated.

        Args:
            project_id: Project ID

        Returns:
            True if locked successfully

        Raises:
            ValueError: If project not found or context already locked
        """
        project = self.db.query(Project).filter(Project.id == project_id).first()
        if not project:
            raise ValueError(f"Project {project_id} not found")

        if project.context_locked:
            logger.warning(f"Project {project_id} context is already locked")
            return True

        if not project.context_semantic:
            raise ValueError(
                f"Cannot lock context for project {project_id}: no context generated yet"
            )

        project.context_locked = True
        project.context_locked_at = datetime.utcnow()
        self.db.commit()

        logger.info(f"üîí Context locked for project {project.name}")

        return True

    def is_context_ready(self, project_id: UUID) -> bool:
        """
        Check if project context is ready (generated and optionally locked).

        Args:
            project_id: Project ID

        Returns:
            True if context_semantic is not empty
        """
        project = self.db.query(Project).filter(Project.id == project_id).first()
        if not project:
            return False

        return bool(project.context_semantic)

    def is_context_locked(self, project_id: UUID) -> bool:
        """
        Check if project context is locked.

        Args:
            project_id: Project ID

        Returns:
            True if context is locked
        """
        project = self.db.query(Project).filter(Project.id == project_id).first()
        if not project:
            return False

        return project.context_locked

    async def activate_suggested_epic(self, epic_id: UUID) -> Dict:
        """
        PROMPT #94 - Activate a suggested item by generating full content.

        Takes a suggested item (with labels=["suggested"] and workflow_state="draft")
        and generates full semantic content using the project context.

        Works with any item type (Epic, Story, Task, Subtask).

        Flow:
        1. Validate item is a suggested item
        2. Fetch project context
        3. Generate full item content using AI (semantic markdown + human description)
        4. Update item with generated content
        5. Remove "suggested" label and change workflow_state to "open"
        6. Lock project context if this is the first activated item (for Epics)

        Args:
            epic_id: Item ID to activate (named epic_id for backwards compatibility)

        Returns:
            Dict with activated item data:
            {
                "id": str,
                "title": str,
                "description": str,
                "generated_prompt": str,
                "semantic_map": Dict,
                "acceptance_criteria": List[str],
                "story_points": int,
                "priority": str,
                "activated": True
            }

        Raises:
            ValueError: If item not found, not suggested, or project has no context
        """
        # 1. Fetch item
        epic = self.db.query(Task).filter(Task.id == epic_id).first()
        if not epic:
            raise ValueError(f"Item {epic_id} not found")

        # Check if it's a suggested item
        is_suggested = (
            epic.labels and "suggested" in epic.labels
        ) or epic.workflow_state == "draft"

        if not is_suggested:
            raise ValueError(
                f"Item {epic_id} is not a suggested item. "
                "It may have already been activated."
            )

        # 2. Fetch project and context
        project = self.db.query(Project).filter(Project.id == epic.project_id).first()
        if not project:
            raise ValueError(f"Project {epic.project_id} not found")

        if not project.context_semantic:
            raise ValueError(
                f"Project {project.id} has no context. "
                "Please complete the Context Interview first."
            )

        # 3. Generate full epic content using AI
        epic_content = await self._generate_full_epic_content(
            project=project,
            epic_title=epic.title,
            epic_description=epic.description
        )

        # 4. Update epic with generated content
        epic.description = epic_content["description"]
        epic.generated_prompt = epic_content["generated_prompt"]
        epic.acceptance_criteria = epic_content.get("acceptance_criteria", [])
        epic.story_points = epic_content.get("story_points")

        # PROMPT #95 - Store complete interview_insights for traceability
        # Includes semantic_map, key_requirements, business_goals, technical_constraints
        epic.interview_insights = epic.interview_insights or {}
        epic.interview_insights["semantic_map"] = epic_content.get("semantic_map", {})
        epic.interview_insights["activated_from_suggestion"] = True
        epic.interview_insights["activation_timestamp"] = datetime.utcnow().isoformat()

        # Merge additional interview_insights from AI response
        ai_insights = epic_content.get("interview_insights", {})
        if ai_insights:
            epic.interview_insights["key_requirements"] = ai_insights.get("key_requirements", [])
            epic.interview_insights["business_goals"] = ai_insights.get("business_goals", [])
            epic.interview_insights["technical_constraints"] = ai_insights.get("technical_constraints", [])

        # 5. Remove "suggested" label and change workflow_state
        if epic.labels and "suggested" in epic.labels:
            epic.labels = [l for l in epic.labels if l != "suggested"]
        epic.workflow_state = "open"
        epic.updated_at = datetime.utcnow()

        # 6. Lock project context (first item activated = context locked)
        if not project.context_locked and epic.item_type == ItemType.EPIC:
            project.context_locked = True
            project.context_locked_at = datetime.utcnow()
            logger.info(f"üîí Context locked for project {project.name} (first item activated)")

        self.db.commit()
        self.db.refresh(epic)

        # PROMPT #95 - Enhanced logging
        logger.info(f"‚úÖ Item activated: {epic.title} ({epic.item_type.value if epic.item_type else 'unknown'})")
        logger.info(f"   - Description: {len(epic.description or '')} chars")
        logger.info(f"   - Description preview: {(epic.description or '')[:300]}...")
        logger.info(f"   - Generated Prompt: {len(epic.generated_prompt or '')} chars")
        logger.info(f"   - Generated Prompt preview: {(epic.generated_prompt or '')[:300]}...")
        logger.info(f"   - Acceptance Criteria: {len(epic.acceptance_criteria or [])} items")
        logger.info(f"   - Story Points: {epic.story_points}")
        logger.info(f"   - Labels: {epic.labels}")
        logger.info(f"   - Workflow State: {epic.workflow_state}")
        logger.info(f"   - Interview Insights keys: {list(epic.interview_insights.keys()) if epic.interview_insights else []}")
        if epic.interview_insights:
            logger.info(f"   - Key Requirements: {len(epic.interview_insights.get('key_requirements', []))} items")
            logger.info(f"   - Business Goals: {len(epic.interview_insights.get('business_goals', []))} items")
            logger.info(f"   - Technical Constraints: {len(epic.interview_insights.get('technical_constraints', []))} items")

        return {
            "id": str(epic.id),
            "title": epic.title,
            "description": epic.description,
            "generated_prompt": epic.generated_prompt,
            "semantic_map": epic_content.get("semantic_map", {}),
            "acceptance_criteria": epic.acceptance_criteria,
            "story_points": epic.story_points,
            "priority": epic.priority.value if epic.priority else "medium",
            "activated": True
        }

    async def _generate_full_epic_content(
        self,
        project: Project,
        epic_title: str,
        epic_description: str
    ) -> Dict:
        """
        Generate full epic content using AI and project context.

        Uses PROMPT #83 Semantic References Methodology to generate:
        - Semantic markdown (generated_prompt) for AI consumption
        - Human description for reading
        - Acceptance criteria
        - Story points estimation
        - Interview insights (key requirements, business goals, technical constraints)

        PROMPT #95 - Enhanced to match the rich structure from Epic Interview flow.

        Args:
            project: Project instance with context
            epic_title: Epic title (from suggested epic)
            epic_description: Epic minimal description (from suggested epic)

        Returns:
            Dict with full epic content
        """
        # PROMPT #95 - Use the same rich structure as backlog_generator.py
        system_prompt = """Voc√™ √© um Product Owner especialista analisando contexto de projeto para gerar Epics completos.

METODOLOGIA DE REFER√äNCIAS SEM√ÇNTICAS:

Esta metodologia funciona da seguinte forma:

1. O texto principal utiliza **identificadores simb√≥licos** (ex: N1, N2, P1, E1, D1, S1, C1) como **refer√™ncias sem√¢nticas**
2. Esses identificadores **N√ÉO s√£o vari√°veis, exemplos ou placeholders**
3. Cada identificador possui um **significado √∫nico e imut√°vel** definido em um **Mapa Sem√¢ntico**
4. O texto narrativo deve ser interpretado **exclusivamente** com base nessas defini√ß√µes
5. **N√£o fa√ßa infer√™ncias** fora do que est√° explicitamente definido no Mapa Sem√¢ntico
6. **N√£o substitua** os identificadores por seus significados no texto
7. Caso haja ambiguidade, ela deve ser apontada, n√£o resolvida automaticamente
8. Caso seja necess√°rio criar novos conceitos, eles devem ser introduzidos como novos identificadores e definidos separadamente

**Categorias de Identificadores:**
- **N** (Nouns/Entidades): N1, N2, N3... = Usu√°rios, sistemas, entidades de dom√≠nio
- **P** (Processes/Processos): P1, P2, P3... = Processos de neg√≥cio, fluxos, workflows
- **E** (Endpoints): E1, E2, E3... = APIs, rotas, endpoints
- **D** (Data/Dados): D1, D2, D3... = Tabelas, estruturas de dados, schemas
- **S** (Services/Servi√ßos): S1, S2, S3... = Servi√ßos, integra√ß√µes, bibliotecas
- **C** (Constraints/Crit√©rios): C1, C2, C3... = Regras de neg√≥cio, valida√ß√µes, restri√ß√µes
- **AC** (Acceptance Criteria): AC1, AC2, AC3... = Crit√©rios de aceita√ß√£o numerados
- **F** (Features/Funcionalidades): F1, F2, F3... = Funcionalidades espec√≠ficas
- **M** (Metrics/M√©tricas): M1, M2, M3... = M√©tricas, KPIs, indicadores

**Objetivo desta metodologia:**
- Reduzir ambiguidade sem√¢ntica
- Manter consist√™ncia conceitual
- Permitir edi√ß√£o posterior manual do c√≥digo
- Garantir rastreabilidade entre texto e implementa√ß√£o

Sua tarefa:
1. Analise o contexto do projeto e o √©pico sugerido
2. Crie um **Mapa Sem√¢ntico** definindo TODOS os identificadores usados (m√≠nimo 15-20 identificadores)
3. Escreva a narrativa completa do Epic usando APENAS esses identificadores
4. Extraia crit√©rios de aceita√ß√£o claros (usando identificadores AC1, AC2, AC3...)
5. Extraia insights chave: requisitos, objetivos de neg√≥cio, restri√ß√µes t√©cnicas
6. Estime story points (1-21, escala Fibonacci) baseado na complexidade do Epic
7. Sugira prioridade (critical, high, medium, low, trivial)

IMPORTANTE:
- Um Epic representa um grande corpo de trabalho (m√∫ltiplas Stories)
- Foque em VALOR DE NEG√ìCIO e RESULTADOS PARA O USU√ÅRIO
- Use identificadores sem√¢nticos em TODO o texto (narrativa, crit√©rios, insights)
- Seja espec√≠fico e acion√°vel nos crit√©rios de aceita√ß√£o
- TUDO DEVE SER EM PORTUGU√äS (t√≠tulo, descri√ß√£o, crit√©rios, identificadores)
- A descri√ß√£o deve ser RICA e DETALHADA (m√≠nimo 800 caracteres)

Retorne APENAS JSON v√°lido (sem markdown code blocks, sem explica√ß√£o):
{
    "title": "T√≠tulo do Epic (conciso, focado em neg√≥cio) - EM PORTUGU√äS",
    "semantic_map": {
        "N1": "Defini√ß√£o clara da entidade 1",
        "N2": "Defini√ß√£o clara da entidade 2",
        "P1": "Defini√ß√£o clara do processo 1",
        "E1": "Defini√ß√£o clara do endpoint 1",
        "D1": "Defini√ß√£o clara da estrutura de dados 1",
        "S1": "Defini√ß√£o clara do servi√ßo 1",
        "C1": "Defini√ß√£o clara do crit√©rio/regra 1",
        "AC1": "Crit√©rio de aceita√ß√£o 1",
        "AC2": "Crit√©rio de aceita√ß√£o 2"
    },
    "description_markdown": "# Epic: [T√≠tulo]\\n\\n## Mapa Sem√¢ntico\\n\\n- **N1**: [defini√ß√£o]\\n- **N2**: [defini√ß√£o]\\n- **P1**: [defini√ß√£o]\\n...\\n\\n## Descri√ß√£o\\n\\n[Narrativa usando APENAS identificadores do mapa sem√¢ntico. Ex: 'Este Epic implementa P1 para N1, permitindo que N2 gerencie D1 via E1.']\\n\\n## Crit√©rios de Aceita√ß√£o\\n\\n1. **AC1**: [crit√©rio usando identificadores]\\n2. **AC2**: [crit√©rio usando identificadores]\\n...\\n\\n## Insights da Entrevista\\n\\n**Requisitos-Chave:**\\n- [requisito usando identificadores]\\n...\\n\\n**Objetivos de Neg√≥cio:**\\n- [objetivo usando identificadores]\\n...\\n\\n**Restri√ß√µes T√©cnicas:**\\n- [restri√ß√£o usando identificadores]\\n...",
    "story_points": 13,
    "priority": "high",
    "acceptance_criteria": [
        "AC1: [Crit√©rio espec√≠fico mensur√°vel usando identificadores sem√¢nticos]",
        "AC2: [Crit√©rio espec√≠fico mensur√°vel usando identificadores sem√¢nticos]",
        "AC3: [Crit√©rio espec√≠fico mensur√°vel usando identificadores sem√¢nticos]"
    ],
    "interview_insights": {
        "key_requirements": ["[requisito usando identificadores]", "[requisito usando identificadores]"],
        "business_goals": ["[objetivo usando identificadores]", "[objetivo usando identificadores]"],
        "technical_constraints": ["[restri√ß√£o usando identificadores]", "[restri√ß√£o usando identificadores]"]
    }
}

**REGRAS CR√çTICAS:**
- description_markdown deve conter TODO o conte√∫do formatado em Markdown
- O Mapa Sem√¢ntico deve estar TANTO no description_markdown quanto no campo semantic_map do JSON
- Use identificadores sem√¢nticos em TODOS os textos (title pode ser em linguagem natural, mas description/criteria/insights devem usar identificadores)
- NUNCA substitua identificadores por seus significados - mantenha sempre os identificadores no texto
- A se√ß√£o "Insights da Entrevista" √© OBRIGAT√ìRIA com Requisitos-Chave, Objetivos de Neg√≥cio e Restri√ß√µes T√©cnicas
"""

        user_prompt = f"""Gere o conte√∫do completo para este Epic sugerido usando a Metodologia de Refer√™ncias Sem√¢nticas.

## CONTEXTO DO PROJETO
**Nome:** {project.name}
**Descri√ß√£o:** {project.description or 'N√£o especificada'}

**Contexto Sem√¢ntico do Projeto (USE ESTES IDENTIFICADORES SE APLIC√ÅVEL):**
{project.context_semantic or 'N√£o dispon√≠vel'}

**Contexto Leg√≠vel do Projeto:**
{project.context_human or 'N√£o dispon√≠vel'}

## EPIC SUGERIDO
**T√≠tulo:** {epic_title}
**Descri√ß√£o Inicial:** {epic_description}

## INSTRU√á√ïES
1. **REUTILIZE** identificadores do contexto sem√¢ntico do projeto quando aplic√°vel
2. **ESTENDA** o mapa com novos identificadores espec√≠ficos para este Epic
3. Crie um Mapa Sem√¢ntico COMPLETO (m√≠nimo 15-20 identificadores)
4. Gere uma descri√ß√£o RICA e DETALHADA usando identificadores sem√¢nticos
5. Defina crit√©rios de aceita√ß√£o claros e mensur√°veis (AC1, AC2, AC3...)
6. Inclua se√ß√£o de Insights com Requisitos-Chave, Objetivos de Neg√≥cio e Restri√ß√µes T√©cnicas
7. Estime story points baseado na complexidade

LEMBRE-SE:
- TODO O CONTE√öDO DEVE SER EM PORTUGU√äS
- Use identificadores sem√¢nticos em TODA a narrativa
- NUNCA substitua identificadores por seus significados
- A descri√ß√£o deve ter M√çNIMO 800 caracteres
- A se√ß√£o de Insights da Entrevista √© OBRIGAT√ìRIA

Retorne o Epic completo como JSON seguindo EXATAMENTE o schema fornecido no system prompt."""

        # Call AI - PROMPT #95: Increased max_tokens to 6000 for richer content
        messages = [{"role": "user", "content": user_prompt}]

        response = await self.orchestrator.execute(
            usage_type="prompt_generation",
            messages=messages,
            system_prompt=system_prompt,
            max_tokens=6000  # Increased from 4000 to allow for richer content
        )

        # Parse response - PROMPT #95: Enhanced JSON extraction
        response_text = response.get("content", "")
        original_response = response_text  # Keep original for debugging
        logger.info(f"üì• Raw AI response length: {len(response_text)} chars")

        # Step 0: Try parsing raw response before any transformation
        result = None
        parse_method = "none"

        try:
            result = json.loads(response_text)
            parse_method = "raw_direct"
            logger.info("‚úÖ JSON parsed from raw response directly")
        except json.JSONDecodeError as e:
            logger.debug(f"Raw parse failed: {e}")

        # Step 1: Strip markdown code blocks
        if result is None:
            response_text = _strip_markdown_json(response_text)

        # Step 2: Try multiple JSON extraction strategies

        # Strategy 1: Direct JSON parse after strip
        if result is None:
            try:
                result = json.loads(response_text)
                parse_method = "direct"
                logger.info("‚úÖ JSON parsed directly after strip")
            except json.JSONDecodeError as e:
                logger.debug(f"Direct parse failed: {e}")

        # Strategy 2: Extract JSON object with regex (greedy)
        if result is None:
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if json_match:
                try:
                    result = json.loads(json_match.group(0))
                    parse_method = "regex_greedy"
                    logger.info("‚úÖ JSON extracted with greedy regex")
                except json.JSONDecodeError:
                    pass

        # Strategy 3: Find balanced braces (handles nested objects)
        if result is None:
            brace_start = response_text.find('{')
            if brace_start != -1:
                brace_count = 0
                brace_end = brace_start
                for i, char in enumerate(response_text[brace_start:], start=brace_start):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            brace_end = i + 1
                            break
                if brace_end > brace_start:
                    try:
                        result = json.loads(response_text[brace_start:brace_end])
                        parse_method = "balanced_braces"
                        logger.info("‚úÖ JSON extracted with balanced braces")
                    except json.JSONDecodeError:
                        pass

        # Strategy 4: Try to fix common JSON issues
        if result is None:
            # Remove trailing commas before closing braces/brackets
            fixed_text = re.sub(r',\s*([}\]])', r'\1', response_text)
            # Try parsing fixed text
            json_match = re.search(r'\{[\s\S]*\}', fixed_text)
            if json_match:
                try:
                    result = json.loads(json_match.group(0))
                    parse_method = "fixed_trailing_commas"
                    logger.info("‚úÖ JSON parsed after fixing trailing commas")
                except json.JSONDecodeError as e:
                    logger.debug(f"Trailing comma fix failed: {e}")

        # Strategy 5: Fix unescaped newlines in JSON strings
        if result is None:
            # This is a common issue where AI returns JSON with literal newlines in strings
            # instead of \n escape sequences
            try:
                # Replace literal newlines inside strings with \n
                # This is a heuristic approach - find strings and escape their newlines
                fixed_text = response_text

                # First, try to find the JSON object
                json_match = re.search(r'\{[\s\S]*\}', fixed_text)
                if json_match:
                    json_str = json_match.group(0)

                    # Try to fix common issues:
                    # 1. Replace literal \n with \\n in strings (already escaped but not properly)
                    # 2. Try to load with strict=False

                    # Attempt 1: Replace problematic characters
                    json_str_fixed = json_str.replace('\r\n', '\\n').replace('\r', '\\n')

                    try:
                        result = json.loads(json_str_fixed)
                        parse_method = "fixed_newlines"
                        logger.info("‚úÖ JSON parsed after fixing newlines")
                    except json.JSONDecodeError:
                        # Attempt 2: More aggressive fix - escape all unescaped newlines
                        # Find all strings and properly escape them
                        pass

            except Exception as e:
                logger.debug(f"Newline fix failed: {e}")

        # Strategy 6: Last resort - try Python's ast.literal_eval for simple cases
        if result is None:
            try:
                import ast
                # This can handle some cases where json.loads fails
                result = ast.literal_eval(response_text)
                if isinstance(result, dict):
                    parse_method = "ast_literal_eval"
                    logger.info("‚úÖ JSON parsed with ast.literal_eval")
                else:
                    result = None
            except:
                pass

        if result:
            logger.info(f"‚úÖ AI response parsed successfully (method: {parse_method})")
            logger.info(f"   - title: {result.get('title', 'N/A')}")
            logger.info(f"   - semantic_map keys: {list(result.get('semantic_map', {}).keys())}")
            logger.info(f"   - description_markdown length: {len(result.get('description_markdown', ''))}")
            logger.info(f"   - acceptance_criteria count: {len(result.get('acceptance_criteria', []))}")
            logger.info(f"   - story_points: {result.get('story_points', 'N/A')}")
            logger.info(f"   - interview_insights keys: {list(result.get('interview_insights', {}).keys())}")
        else:
            # All parsing strategies failed
            logger.error(f"‚ùå Failed to parse AI response as JSON after all strategies")
            logger.error(f"Response text (first 1500 chars): {response_text[:1500]}...")

            # Fallback: create meaningful content from the epic data and project context
            # PROMPT #95 - Enhanced fallback with rich structure
            logger.warning("Using fallback content generation...")

            # Build a meaningful description from the context with Semantic References structure
            fallback_description = f"""# Epic: {epic_title}

## Mapa Sem√¢ntico

- **N1**: {project.name}
- **E1**: {epic_title}
- **P1**: Processo principal de implementa√ß√£o
- **D1**: Dados e estruturas do m√≥dulo
- **S1**: Servi√ßos e integra√ß√µes necess√°rias
- **C1**: Funcionalidades devem estar completas
- **C2**: C√≥digo deve seguir padr√µes de qualidade
- **AC1**: E1 deve estar completamente implementado
- **AC2**: D1 deve estar corretamente estruturado
- **AC3**: S1 deve estar integrado com o sistema

## Descri√ß√£o

Este Epic implementa E1 como parte de N1, seguindo P1 para garantir a entrega de valor ao usu√°rio. O m√≥dulo gerencia D1 e integra S1 para fornecer as funcionalidades necess√°rias.

{epic_description}

O desenvolvimento segue C1 e C2 para garantir qualidade e consist√™ncia com o restante do sistema.

## Crit√©rios de Aceita√ß√£o

1. **AC1**: E1 deve estar completamente implementado com todas as funcionalidades descritas
2. **AC2**: D1 deve estar corretamente estruturado e validado
3. **AC3**: S1 deve estar integrado e funcionando com os demais m√≥dulos

## Insights da Entrevista

**Requisitos-Chave:**
- E1 deve atender aos requisitos de neg√≥cio de N1
- P1 deve seguir as melhores pr√°ticas de desenvolvimento
- D1 deve estar bem documentado

**Objetivos de Neg√≥cio:**
- Entregar E1 com valor ao usu√°rio final
- Garantir escalabilidade de S1
- Manter qualidade conforme C1 e C2

**Restri√ß√µes T√©cnicas:**
- E1 deve ser compat√≠vel com a arquitetura existente
- D1 deve seguir os padr√µes de dados do projeto
- S1 deve ter performance adequada
"""

            result = {
                "title": epic_title,
                "semantic_map": {
                    "N1": project.name,
                    "E1": epic_title,
                    "P1": "Processo principal de implementa√ß√£o",
                    "D1": "Dados e estruturas do m√≥dulo",
                    "S1": "Servi√ßos e integra√ß√µes necess√°rias",
                    "C1": "Funcionalidades devem estar completas",
                    "C2": "C√≥digo deve seguir padr√µes de qualidade",
                    "AC1": "E1 deve estar completamente implementado",
                    "AC2": "D1 deve estar corretamente estruturado",
                    "AC3": "S1 deve estar integrado com o sistema"
                },
                "description_markdown": fallback_description,
                "acceptance_criteria": [
                    "AC1: E1 deve estar completamente implementado com todas as funcionalidades descritas",
                    "AC2: D1 deve estar corretamente estruturado e validado",
                    "AC3: S1 deve estar integrado e funcionando com os demais m√≥dulos"
                ],
                "story_points": 13,
                "interview_insights": {
                    "key_requirements": [
                        "E1 deve atender aos requisitos de neg√≥cio de N1",
                        "P1 deve seguir as melhores pr√°ticas de desenvolvimento",
                        "D1 deve estar bem documentado"
                    ],
                    "business_goals": [
                        "Entregar E1 com valor ao usu√°rio final",
                        "Garantir escalabilidade de S1",
                        "Manter qualidade conforme C1 e C2"
                    ],
                    "technical_constraints": [
                        "E1 deve ser compat√≠vel com a arquitetura existente",
                        "D1 deve seguir os padr√µes de dados do projeto",
                        "S1 deve ter performance adequada"
                    ]
                }
            }

        # Extract and process content
        semantic_map = result.get("semantic_map", {})
        description_markdown = result.get("description_markdown", "")

        # generated_prompt = semantic markdown (for AI/child cards)
        generated_prompt = description_markdown

        # description = human-readable (converted from semantic)
        description = _convert_semantic_to_human(description_markdown, semantic_map)

        # Remove Mapa Sem√¢ntico section from human description
        description = re.sub(
            r'##\s*Mapa\s*Sem[a√¢]ntico\s*\n+(?:[-*]\s*\*\*[^*]+\*\*:[^\n]*\n*)*',
            '',
            description,
            flags=re.IGNORECASE | re.MULTILINE
        )
        description = description.strip()

        # PROMPT #95 - Include interview_insights in return
        return {
            "title": result.get("title", epic_title),
            "description": description,
            "generated_prompt": generated_prompt,
            "semantic_map": semantic_map,
            "acceptance_criteria": result.get("acceptance_criteria", []),
            "story_points": result.get("story_points"),
            "interview_insights": result.get("interview_insights", {})
        }

    async def reject_suggested_epic(self, epic_id: UUID) -> bool:
        """
        PROMPT #94 - Reject (delete) a suggested item.

        Works with any item type (Epic, Story, Task, Subtask).

        Args:
            epic_id: Item ID to reject (named epic_id for backwards compatibility)

        Returns:
            True if deleted successfully

        Raises:
            ValueError: If item not found or not a suggested item
        """
        # Fetch item
        epic = self.db.query(Task).filter(Task.id == epic_id).first()
        if not epic:
            raise ValueError(f"Item {epic_id} not found")

        # Check if it's a suggested item
        is_suggested = (
            epic.labels and "suggested" in epic.labels
        ) or epic.workflow_state == "draft"

        if not is_suggested:
            raise ValueError(
                f"Item {epic_id} is not a suggested item. "
                "Only suggested items can be rejected."
            )

        item_title = epic.title

        # Delete the item
        self.db.delete(epic)
        self.db.commit()

        logger.info(f"‚ùå Suggested item rejected and deleted: {item_title}")

        return True
