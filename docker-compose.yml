services:
  # =============================================================================
  # CORE ORBIT SERVICES
  # =============================================================================

  # PostgreSQL Database (with pgvector extension)
  postgres:
    image: pgvector/pgvector:pg16
    container_name: orbit-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-orbit}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-orbit_password}
      POSTGRES_DB: ${POSTGRES_DB:-orbit}
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./docker/init-db.sh:/docker-entrypoint-initdb.d/init-db.sh:ro
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-orbit} -d ${POSTGRES_DB:-orbit}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - orbit-network

  # Backend FastAPI
  backend:
    build:
      context: .
      dockerfile: docker/backend.Dockerfile
    container_name: orbit-backend
    restart: unless-stopped
    environment:
      DATABASE_URL: ${DATABASE_URL:-postgresql://orbit:orbit_password@postgres:5432/orbit}
      SECRET_KEY: ${SECRET_KEY:-dev-secret-key-change-in-production}
      ENVIRONMENT: ${ENVIRONMENT:-development}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      GOOGLE_AI_API_KEY: ${GOOGLE_AI_API_KEY:-}
      DEBUG: ${DEBUG:-True}
      LOG_LEVEL: ${LOG_LEVEL:-INFO}
      # CORS_ORIGINS: Removed - using default from backend/app/config.py
      # Prompter Feature Flags
      PROMPTER_USE_TEMPLATES: ${PROMPTER_USE_TEMPLATES:-true}
      PROMPTER_USE_STRUCTURED_TEMPLATES: ${PROMPTER_USE_STRUCTURED_TEMPLATES:-true}
      PROMPTER_USE_CACHE: ${PROMPTER_USE_CACHE:-true}
      PROMPTER_USE_BATCHING: ${PROMPTER_USE_BATCHING:-true}
      # Infrastructure connections
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
    volumes:
      - ./backend:/app
      - /app/.venv
      - ./projects:/projects
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - orbit-network
    command: >
      sh -c "
        echo 'Waiting for database...' &&
        sleep 5 &&
        poetry run alembic upgrade head &&
        poetry run uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
      "

  # Frontend Next.js (Development mode)
  frontend:
    build:
      context: .
      dockerfile: docker/frontend.Dockerfile
      target: deps
    container_name: orbit-frontend
    restart: unless-stopped
    environment:
      NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      NEXT_PUBLIC_APP_NAME: ${NEXT_PUBLIC_APP_NAME:-Orbit}
      NODE_ENV: development
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /app/.next
    ports:
      - "3000:3000"
    depends_on:
      - backend
    networks:
      - orbit-network
    command: npm run dev

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: orbit-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - orbit-network

  # =============================================================================
  # RAG / LOCAL AI SERVICES (Ollama + Qdrant + PostgreSQL for RAG)
  # =============================================================================
  # These services use BIND MOUNTS for persistence in ./data/
  # All data is stored in explicit host folders (not Docker named volumes)

  # Ollama - Local LLM Server
  # Docs: https://ollama.com/
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      # Bind mount for persistence - models stored in ./data/ollama
      - ./data/ollama:/root/.ollama
    networks:
      - orbit-network
    # ==========================================================================
    # GPU NVIDIA SUPPORT (desabilitado por padr√£o)
    # ==========================================================================
    # Para habilitar GPU NVIDIA, descomente as linhas abaixo:
    #
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all  # ou "1" para usar apenas 1 GPU
    #           capabilities: [gpu]
    #
    # Requisitos para GPU:
    # 1. NVIDIA Driver instalado no host
    # 2. NVIDIA Container Toolkit instalado:
    #    curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
    #    curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    #      sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    #      sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
    #    sudo apt-get update && sudo apt-get install -y nvidia-container-toolkit
    #    sudo nvidia-ctk runtime configure --runtime=docker
    #    sudo systemctl restart docker
    # 3. Verificar com: docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi
    # ==========================================================================

  # Qdrant - Vector Database
  # Docs: https://qdrant.tech/documentation/
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # REST API
      - "6334:6334"  # gRPC API
    volumes:
      # Bind mount for persistence - vectors stored in ./data/qdrant
      - ./data/qdrant:/qdrant/storage
    networks:
      - orbit-network
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334

volumes:
  postgres_data:
    driver: local
  redis-data:
    driver: local

networks:
  orbit-network:
    driver: bridge
